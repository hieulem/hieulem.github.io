<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
  <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <link rel="stylesheet" href="files/jemdoc.css" type="text/css" />

  <title>Hieu Le</title>

</head>

<head>
  <style>
    div.scroll {
      max-height: 200px;
      overflow: scroll;
    }
  </style>
  <style>
  .proj_thumb {
    height: 70px;
    width: 70px;
    object-fit: scale-down;
    border-radius: 4px;
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.08);
    transition: transform 0.3s ease, box-shadow 0.3s ease;
    margin-right: 0px;
  }
  .proj_thumb:hover {
    transform: scale(1.5);
    box-shadow: 0 6px 12px rgba(0, 0, 0, 0.15);
  }
  </style>
  <style>
    /* Style the tab buttons */
    .tab {
      display: flex;
      background-color: #f1f1f1;
      border-bottom: 1px solid #ccc;
    }

    .tab button {
      background-color: inherit;
      border: 1px;
      outline: none;
      cursor: pointer;
      padding: 10px 5px;
      transition: 0.3s;
      font-size: 16px;
    }

    .tab button:hover {
      background-color: #ddd;
    }

    .tab button.active {
      background-color: #ccc;
    }

    /* Style the tab content */
    .tab-content {
      display: none;
      padding: 0px;
      border: 0px solid #ccc;
      border-top: 10px;
    }

    .tab-content.active {
      display: block;
    }
    .spotlight-button {
      background-color: #009dff;
      /* Green background */
      color: rgb(255, 255, 255);
      /* White text */
      border: none;
      /* Remove borders */
      padding: 2px 2px;
      /* Add padding */
      text-align: center;
      /* Center the text */
      text-decoration: none;
      /* Remove underline */
      display: inline-block;
      /* Inline-block element */
      font-size: 14px;
      /* Font size */
      margin-left: 5px;
      /* Spacing next to the Arxiv link */
      /* Pointer cursor on hover */
      border-radius: 1px;
      /* Rounded corners */
      transition: background-color 0.3s;
      /* Smooth hover effect */
    }

    .nice-button {
      background-color: #4CAF50;
      /* Green background */
      color: white;
      /* White text */
      border: none;
      /* Remove borders */
      padding: 2px 2px;
      /* Add padding */
      text-align: center;
      /* Center the text */
      text-decoration: none;
      /* Remove underline */
      display: inline-block;
      /* Inline-block element */
      font-size: 14px;
      /* Font size */
      margin-left: 10px;
      /* Spacing next to the Arxiv link */
      cursor: pointer;
      /* Pointer cursor on hover */
      border-radius: 2px;
      /* Rounded corners */
      transition: background-color 0.3s;
      /* Smooth hover effect */
    }

    .nice-button:hover {
      background-color: #45a049;
      /* Darker green on hover */
    }
  </style>
</head>

<body>

  <!-- Project
<div class="menu"> <a href="#home">Home</a> 
<a href="#publications">Publications</a> 
<a href="#services">Services</a> 
<a href="#awards">Awards</a>  
</div>
 -->

  <a id="home" class="anchor"></a>
  <div id="container">
    <div class="container">
      <!--
<div id="toptitle">
<h1>Hieu Le</h1>
</div>
 -->


      <table class="imgtable">
        <tr>
          <td>
            <a href="./"><img src="./files/hieu.JPG" alt="" height="200px" /></a>&nbsp;
          </td>
          <td align="left">
            <p>
              <font size="4">Hieu Le (Minh Hiếu Lê)</font><br />
              <br />

              <a href="https://www.epfl.ch/labs/cvlab/">Computer Vision Lab - EPFL</a><br />
              BC300, Faculté Informatique et Communications – IC <br />
              Ecole Polytechnique Fédérale de Lausanne – EPFL
              <br />
              <br />


              Email: hle@cs.stonybrook.edu <br />
              [<a class="p1" href="https://scholar.google.com/citations?user=Bj9g-EEAAAAJ&hl=en" target="_blank">Google
                Scholar</a>] [<a class="p2" href="https://github.com/hieulem" target="_blank">Github</a>]
          </td>
        </tr>
      </table>


      <p> I am a postdoc at CVLab EPFL working with Prof. <a href="https://people.epfl.ch/pascal.fua?lang=en"
          target="_blank">Pascal Fua</a> and <a href=https://people.epfl.ch/mathieu.salzmann>Mathieu Salzmann</a>. Before that, I obtained my Ph.D. in computer science from Stony
        Brook Univeristy working with
        Prof. <a href="https://www3.cs.stonybrook.edu/~samaras/" target="_blank">Dimitris Samaras</a> and spent two years as an applied
        scientist at Amazon Robotics - Boston.</p>
      
      </p>

      <h2>News</h2>
      <ul>
        <div class="scroll">
          <li>
            <p><b>[2025]</b> Pair-wise implicit constraint has been early accepted (top 9%) to <b>MICCAI 2025</b>.</p>
          </li>
          <li>
            <p><b>[2025]</b> Qt-DoG &#128054; has been accepted to <b>ICML 2025</b>.</p>
          </li>
          <li>
            <p><b>[2025]</b> A paper on personalized scanpath prediction has been accepted to <b>CVPR 2025</b>.</p>
          </li>
          <li>
            <p><b>[2025]</b> A paper on identifying <a href="https://arxiv.org/pdf/2411.18810">reliable seeds</a> for diffusion models has been accepted to <b>ICLR 2025</b> as a Spotlight paper.</p>
          </li>
          <li>
            <p><b>[2024]</b> Two papers accepted to <b>WACV 2025</b>:
              <a href="https://arxiv.org/abs/2409.06848">Material-Consistent Shadow Edges</a>, and
              <a href="https://arxiv.org/pdf/2307.07677">Multi-Class Object Counting</a>.
            </p>
          </li>
          <li>
            <p><b>[2024]</b> <a href="https://openreview.net/forum?id=8Vk1Bmg3sY">Pseudo-Density</a> has been accepted
              to <b>TMLR</b>. Similar to
              <a href="https://hieulem.github.io/pages/gqa/">Latent Density Score</a>, this paper reveals the learning
              mechanism of generative models.
            </p>
          </li>
          <li>
            <p><b>[2024]</b> Four papers accepted to <b>ECCV 2024</b>:
              <a href="https://hieulem.github.io/pages/gqa/">Latent Density Score</a>,
              <a href="https://ilceltico.github.io/nsdudf/">Neural Surface Localization</a>,
              <a href="https://arxiv.org/pdf/2407.12630">Pseudo-Pixel Weighting</a>, and
              <a href="https://arxiv.org/abs/2407.04036">Beyond Pixels</a>.
            </p>
          </li>
          <li>
            <p><b>[2024]</b> A paper on 3D heart generation has been accepted to <b>MICCAI 2024</b>.</p>
          </li>
          <li>
            <p><b>[2024]</b> Our paper on uncertainty estimation has been accepted to <b>ICML 2024</b>.</p>
          </li>
          <li>
            <p><b>[2024]</b> <i>Zigzag</i> has been accepted by <b>TMLR</b>. Zigzag is a universal uncertainty
              estimation method that requires only two inference passes.</p>
          </li>
          <li>
            <p><b>[2023]</b> Two papers accepted to <b>CVPR 2023</b>: Zero-Shot Object Counting and Few-Shot Object
              Detection.</p>
          </li>
          <li>
            <p><b>[2023]</b> Selected as an outstanding reviewer at
              <a href="https://www.accv2022.org/en/OUTSTANDING-REVIEWERS.html" target="_blank">ACCV 2022</a>.
            </p>
          </li>
          <li>
            <p><b>[2022]</b> Our paper on Few-Shot Classification via Generating Representative Prototypes has been
              accepted to <b>CVPR 2022</b>.</p>
          </li>
          <li>
            <p><b>[2022]</b> Our journal paper on Physics-Based Shadow Removal has been accepted by <b>TPAMI 2022</b>.
            </p>
          </li>
          <li>
            <p><b>[2022]</b> Our journal paper on detecting penguin colonies from satellite images has been accepted by
              <b>Remote Sensing 2022</b>.
            </p>
          </li>
          <li>
            <p><b>[2021]</b> Our paper on Few-Shot Classification has been accepted to <b>ICCV 2021</b>.</p>
          </li>
          <li>
            <p><b>[2021]</b> Outstanding Reviewer -
              <a href="http://cvpr2021.thecvf.com/node/184" target="_blank">CVPR 2021</a>.
            </p>
          </li>
          <li>
            <p><b>[2020]</b> Ph.D. completed.</p>
          </li>
          <li>
            <p><b>[2020]</b> Outstanding Reviewer -
              <a href="https://eccv2020.eu/outstanding-reviewers/" target="_blank">ECCV 2020</a>.
            </p>
          </li>
          <li>
            <p><b>[2020]</b> Our paper on Weakly-Supervised Shadow Removal has been accepted to <b>ECCV 2020</b>.
              <a href="https://www3.cs.stonybrook.edu/~cvl/projects/FSS2SR/index.html" target="_blank">Project Page</a>.
            </p>
          </li>
          <li>
            <p><b>[2019]</b> Our paper on Shadow Removal has been accepted to <b>ICCV 2019</b>.
              <a href="https://www3.cs.stonybrook.edu/~cvl/projects/SID/index.html" target="_blank">Project Page</a>.
            </p>
          </li>
          <li>
            <p><b>[2019]</b> Our paper on Semi-Supervised Segmentation has been accepted to <b>CVPR 2019 - CV4GC</b>.
            </p>
          </li>
          <li>
            <p><b>[2019]</b> Our paper on Policy Mining Using Neural Networks has been accepted to <b>SACMAT 2019</b>.
            </p>
          </li>
          <li>
            <p><b>[2018]</b> Two papers accepted to <b>ECCV 2018</b>:
              <a href="http://www3.cs.stonybrook.edu/~cvl/projects/adnet/index.html" target="_blank">ADNet</a> for
              shadow detection and Iterative Crowd Counting.
            </p>
          </li>
          <li>
            <p><b>[2017]</b> Our paper on Object Co-Localization has been accepted to <b>ICCV 2017</b>, Workshop on
              CEFRL, Venice, Italy.</p>
          </li>
          <li>
            <p><b>[2017]</b> Our poster on Object Co-Localization was presented at
              <a href="http://www.visionsciences.org/programs/VSS_2017_Abstracts.pdf" target="_blank">VSS 2017</a>
              (Florida, US).
              <a href="VSS2017.pdf" target="_blank">[Poster]</a>
            </p>
          </li>
          <li>
            <p><b>[2016]</b> Geodesic Features for Video Segmentation has been accepted to <b>ACCV 2016</b>.</p>
          </li>
        </div>
      </ul>



      
      </head>

      <body>

        <h2>Publications</h2>

        <!-- Tab buttons -->
        <div class="tab">
          <button class="tablinks active" onclick="openTab(event, 'selected')">Selected Publications</button>
          <button class="tablinks" onclick="openTab(event, 'preprints')">Selected Preprints</button>
        </div>

        <!-- Tab content -->
        <div id="selected" class="tab-content active">
          <table>
            <table class="imgtable">
              <!--  -->
              <tr>
                <td>
                    <img class="proj_thumb" src="Paper/2025/pairwise-sdf.png" alt="Paper Thumbnail" height="100px" />&nbsp;
                </td>
                <td>
                    <p class="pub_title">Pairwise-Constrained Implicit Functions for 3D Human Heart Modeling</p>
                    <p class="pub_author">
                        <b>Hieu Le</b>, Jingyi Xu, Nicolas Talabot, Jiancheng Yang, Pascal Fua. 2025<br>
                        Medical Image Computing and Computer Assisted Intervention  (<b>MICCAI</b>), 2025. <button class="spotlight-button">Early Accept (top 9%)</button><br>
                        <a href="https://arxiv.org/pdf/2307.08716" target="_blank">[Preprint]</a> 
                        <button class="nice-button" onclick="toggleDescription('tldr-pairwise')">TL;DR</button>
                        
                    </p>
                    <p id="tldr-pairwise" style="display: none; font-style: italic; color: green;">
                        We use monte a carlo sampling method for finding relevant points to refine pairs of SDF surfaces, making them contact each other without penetrating. 
                    </p>
                </td>
             </tr>

              <!--  -->
              <tr>
                <td><img class="proj_thumb" src="Paper/qtdog25.png" alt="Paper Thumbnail" height="100px" />&nbsp;
                </td>
                <td>
                  <p class="pub_title">QT-DoG: Quantization-Aware Training for Domain Generalization</p>
                  <p class="pub_author">
                    Saqib Javed, <b>Hieu Le</b>, Mathieu Salzmann<br>
                    International Conference on Machine Learning (<b>ICML</b>), 2025.<br>
                    <a href="https://arxiv.org/pdf/2410.06020" target="_blank">[Preprint]</a> <a
                      href="https://saqibjaved1.github.io/QT_DoG/">[Project Page]</a> <a
                      href="https://github.com/saqibjaved1/QT-DoG">[Code]</a>
                    <button class="nice-button" onclick="toggleDescription('tldr-qt-dog')">TL;DR</button>
                  </p>
                  <p id="tldr-qt-dog" style="display: none; font-style: italic; color: green;">
                    TL;DR: QT-DoG uses weight quantization as a regularizer to encourage flatter minima, enhancing
                    domain generalization. We also introduce ensembles of quantized models, achieving SoTA performance in DG.
                  </p>
                </td>
              </tr>

              <tr>
                <td>
                    <img class="proj_thumb" src="Paper/2025/fewshot-scanpath.png" alt="Paper Thumbnail" height="100px" />&nbsp;
                </td>
                <td>
                    <p class="pub_title">Few-shot Personalized Scanpath Prediction</p>
                    <p class="pub_author">
                       Ruoyu Xue, Jingyi Xu, Sounak Mondal, <b>Hieu Le</b>, Greg Zelinsky, Minh Hoai, Dimitris Samaras, 2025<br>
                       IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025. <br>
                        <a href="TBD" target="_blank">[Preprint]</a> 
                    
                    </p>

                </td>
             </tr>

              

              <tr>
                <td>
                    <img class="proj_thumb" src="Paper/2025/enhancecompositional2025.png" alt="Paper Thumbnail" height="100px" />&nbsp;
                </td>
                <td>
                    <p class="pub_title">Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds</p>
                    <p class="pub_author">
                        Shuangqi Li, <b>Hieu Le</b>, Jingyi Xu, Mathieu Salzmann, 2025<br>
                        International Conference on Learning Representations (<b>ICLR</b>), 2025. <button class="spotlight-button">Spotlight (top 5%)</button><br>
                        <a href="https://arxiv.org/pdf/2411.18810" target="_blank">[Preprint]</a> 
                        <button class="nice-button" onclick="toggleDescription('tldr-compositional-text-to-image')">TL;DR</button>
                        
                    </p>
                    <p id="tldr-compositional-text-to-image" style="display: none; font-style: italic; color: green;">
                        TL;DR: Noises are important for diffusion-based models. We show that some random seeds are much more reliable than others, which can be used to generate useful training data. 
                    </p>
                </td>
             </tr>

              <tr>
                <td><img class="proj_thumb" src="Paper/WACV25_counting.png" alt="Paper Thumbnail"
                    height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Learning to Count from Pseudo-Labeled Segmentation</p>
                  <p class="pub_author">
                    Jingyi Xu, <b>Hieu Le</b>, Dimitris Samaras<br>
                    Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2025.<br>
                    [<a href="https://arxiv.org/pdf/2307.07677" target="_blank">Preprint</a>]
                    <button class="nice-button" onclick="toggleDescription('tldr-counting')">TL;DR</button>
                  </p>
                  <p id="tldr-counting" style="display: none; font-style: italic; color: #22710e;">
                    Existing methods suffer from the counting-everything issues. We introduce a benchmark with multiple
                    countable objects in each image and show that we can mitigate this issues by using synthetic data.
                  </p>
                </td>
              </tr>


              <tr>
                <td><img class="proj_thumb" src="Paper/WACV25_SR.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Shadow Removal Refinement via Material-Consistent Shadow Edges</p>
                  <p class="pub_author">
                    Shilin Hu, <b>Hieu Le</b>, ShahRukh Athar, Sagnik Das, Dimitris Samaras<br>
                    Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2025.<br>
                    [<a href="https://arxiv.org/abs/2409.06848" target="_blank">Preprint</a>][<a href="https://github.com/cvlab-stonybrook/ShadowRemovalRefine"> Project Page</a>] [<a href="https://drive.google.com/drive/folders/1nUyo95g80kkRBOiU2YfmQUi50-TIUTGm"> Testing Set</a> ]
                    <button class="nice-button" onclick="toggleDescription('desc-shadow-removal')">TL;DR</button>
                  </p>
                  <p id="desc-shadow-removal" style="display: none; font-style: italic; color: #22710e;">
                    We proposed a new shadow removal method leveraging supervision from the shadow edges. Plus, we
                    introduced a new benchmark for shadow removal with no shadow-free images needed!
                  </p>
                </td>
              </tr>



              <tr style="background-color: #95fa2939;">
                <td><img class="proj_thumb" src="Paper/ECCV24_QA.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Assessing Sample Quality via the Latent Space of Generative Models </p>
                  <p class="pub_author"> Jingyi Xu, <b>Hieu Le</b>, Dimitris Samaras<br> European Conference on Computer
                    Vision </i> (<b>ECCV</b>), 2024.<br>
                    [<a href="http://arxiv.org/abs/2407.15171" target="_blank"> Preprint</a>] [<a
                      href="https://github.com/cvlab-stonybrook/LS-sample-quality" target="_blank"> Code</a>]
                    [<a href="https://hieulem.github.io/pages/gqa/" target="_blank"> Project Page</a>]
                    <button class="nice-button" onclick="toggleDescription('desc-asq')">TL;DR</button>
                  </p>
                  <p id="desc-asq" style="display: none; font-style: italic; color: #22710e;">
                    Latent points located in dense regions of the latent manifold tend to produce higher-quality
                    samples, while those in sparser regions yield lower-quality ones.
                  </p>
                  </p>
                </td>
              </tr>

              <tr>
                <td><img class="proj_thumb" src="Paper/tmlr24b.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Controlling the Fidelity and Diversity of Deep Generative Models via Pseudo
                    Density </p>
                  <p class="pub_author"> Shuangqi Li, Chen Liu, Tong Zhang, <b>Hieu Le</b>, Sabine Susstrunk, Mathieu
                    Salzmann<br> European Conference on Computer Vision </i> (<b>TMLR</b>), 2024.<br>
                    [<a href="https://openreview.net/forum?id=8Vk1Bmg3sY" target="_blank"> Openreview</a>]
              </tr>

              <tr>
                <td><img class="proj_thumb" src="Paper/media/ECCV24_NSF.gif" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title"> Neural Surface Localization for Unsigned Distance Fields </p>
                  <p class="pub_author"> Federico Stella, Nicolas Talabot, <b>Hieu Le</b>, Pascal Fua <br> European
                    Conference on Computer Vision </i> (<b>ECCV</b>), 2024.<br>
                    [<a href="https://arxiv.org/pdf/2407.18381" target="_blank"> Preprint</a>] [<a
                      href="https://ilceltico.github.io/nsdudf/" target="_blank"> Project Page</a>]
                  </p>
                </td>
              </tr>

              <tr>
                <td><img class="proj_thumb" src="Paper/ECCV24_ODSS.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title"> Weighting Pseudo-Labels via High-Activation Feature Index Similarity and Object
                    Detection for Semi-Supervised Segmentation </p>
                  <p class="pub_author"> Prantik Howlader, <b>Hieu Le</b>, Dimitris Samaras <br> European Conference on
                    Computer Vision </i> (<b>ECCV</b>), 2024.<br>
                    [<a href="https://arxiv.org/pdf/2407.12630" target="_blank"> Preprint</a>] [<a
                      href="https://github.com/cvlab-stonybrook/Weighting-Pseudo-Labels" target="_blank"> Code</a>]
                  </p>
                </td>
              </tr>

              <tr>
                <td><img class="proj_thumb" src="Paper/ECCV24_MLPSS.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title"> Beyond Pixels: Semi-Supervised Semantic Segmentation with a Multi-scale
                    Patch-based Multi-Label Classifier </p>
                  <p class="pub_author"> Prantik Howlader, Srijan Das, <b>Hieu Le</b>, Dimitris Samaras <br> European
                    Conference on Computer Vision </i> (<b>ECCV</b>), 2024.<br>
                    [<a href="https://arxiv.org/pdf/2407.04036" target="_blank"> Preprint</a>] [<a
                      href="https://github.com/cvlab-stonybrook/Beyond-Pixels" target="_blank"> Code</a>]
                  </p>
                </td>
              </tr>
              <!--  -->
              <tr>
                <td>
                  <img class="proj_thumb" src="Paper/ImHeartMICCAI24.jpg" alt="Heart Structures Paper Thumbnail"
                    height="100px" />&nbsp;
                </td>
                <td>
                  <p class="pub_title">
                    Generating Anatomically Accurate Heart Structures via Neural Implicit Fields
                  </p>
                  <p class="pub_author">
                    Jiancheng Yang, Ekaterina Sedykh, Jason Adhinarta, <b>Hieu Le</b>, Pascal Fua<br>
                    Medical Image Computing and Computer Assisted Intervention  (<b>MICCAI</b>), 2024<br>
                    <a href="https://papers.miccai.org/miccai-2024/357-Paper0411.html" target="_blank">[Paper]</a>
                    <a href="https://1drv.ms/b/s!AqZcVxAtazRmkZAz1UskMKvbnRHy4w?e=xA9dax" target="_blank">[Poster]</a>
                  </p>
                </td>
              </tr>
              <!--  -->
              <tr>
                <td><img class="proj_thumb" src="Paper/ICML24.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Enabling Uncertainty Estimation in Iterative Neural Networks </p>
                  <p class="pub_author"> Nikita Durasov, Doruk Oner, Jonathan Donier, <b>Hieu Le</b>, Pascal Fua<br>
                    International Conference on Machine Learning (<b>ICML</b>), 2024.<br>
                    [<a href="https://arxiv.org/pdf/2403.16732" target="_blank"> Preprint</a>] [<a
                      href="https://github.com/cvlab-epfl/TBD" target="_blank"> Code</a>]
                  </p>
                </td>
              </tr>

              <tr>
                <td><img class="proj_thumb" src="Paper/zigzag.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Zigzag: Universal Sampling-free Uncertainty Estimation Through Two-Step Inference
                  </p>
                  <p class="pub_author"> Nikita Durasov,Nik Dorndorf, <b>Hieu Le</b>, Pascal Fua<br> Transactions on
                    Machine Learning Research
                    </i> (<b>TMLR</b>), 2024.<br>
                    [<a href="https://openreview.net/pdf?id=QSvb6jBXML" target="_blank"> Openreview</a>] [<a
                      href="https://github.com/cvlab-epfl/zigzag" target="_blank"> Code</a>]
                  </p>
                </td>
              </tr>


              <!-- CVPR 2023-->
              <tr style="background-color: #95fa2939;">
                <td><img class="proj_thumb" src="Paper/media/ZSC.gif" alt="" height="90px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Zero-Shot Object Counting </p>
                  <p class="pub_author"> Jingyi Xu, <b>Hieu Le</b>, Vu Nguyen, Viresh Ranjan, Dimitris Samaras<br>
                    IEEE Conference on Computer Vision and Pattern Recognition </i> (<b>CVPR</b>), 2023.<br>
                    [<a href="https://github.com/cvlab-stonybrook/zero-shot-counting" target="_blank"> Code</a>] [<a
                      href="https://arxiv.org/pdf/2309.13097.pdf" target="_blank"> Stable-Diffusion extension</a>]
                  </p>
                </td>
              </tr>

              <tr>
                <td><img class="proj_thumb" src="Paper/cvpr23_fsod.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Generating Features With Increased Crop-related Diversity For Few-shot Object
                    Detection </p>
                  <p class="pub_author">Jingyi Xu, <b>Hieu Le</b>, Dimitris Samaras<br>
                    IEEE Conference on Computer Vision and Pattern Recognition </i> (<b>CVPR</b>), 2023.<br>
                    [<a href="https://github.com/cvlab-stonybrook/TBD" target="_blank"> Code (TBD)</a>]
                  </p>
                </td>
              </tr>



              <!-- CVPR 2022-->
              <tr style="background-color: #95fa2939;">
                <td><img class="proj_thumb" src="Paper/CVPR2022.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Generating Representative Samples for Few-Shot Classification </p>
                  <p class="pub_author">Jingyi Xu, <b>Hieu Le</b><br>
                    IEEE Conference on Computer Vision and Pattern Recognition </i> (<b>CVPR</b>), 2022.<br>
                    [<a href="https://github.com/cvlab-stonybrook/fsl-rsvae" target="_blank">Code</a>] [<a
                      href="files/CVPR22_FS-3.pdf">Paper</a> ] [<a
                      href="https://synthesis.ai/2022/07/14/cvpr-22-part-ii-new-use-cases-for-synthetic-data/"> Blog
                      post </a>]
                  </p>
                </td>
              </tr>



              <!-- TPAMI 2022-->
              <tr style="background-color: #95fa2939;">
                <td><img class="proj_thumb" src="Paper/PAMI2022.jpg" alt="" height="80px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Physics-based Shadow Image Decomposition for Shadow Removal </p>
                  <p class="pub_author"> <b>Hieu Le</b>, Dimitris Samaras.<br>
                    IEEE Transactions on Pattern Analysis and Machine Intelligence </i> (<b>TPAMI</b>), 2022.<br>
                    [<a href="https://github.com/cvlab-stonybrook/SID">Github</a> ] [<a
                      href="https://arxiv.org/abs/2012.13018">Paper</a> ]


                  </p>
                </td>
              </tr>

              <!-- Remote Sensing 2022-->
              <tr>
                <td><img class="proj_thumb" src="Paper/RS2022.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">A convolutional neural network architecture designed for the automated survey of
                    seabird colonies </p>
                  <p class="pub_author"> <b>Hieu Le</b>, Dimitris Samaras, Heather J. Lynch <br>
                    Remote Sensing in Ecology and Conservation </i> , 2022.<br>
                    [<a href="files/RS2022.pdf">Paper</a> ]
                  </p>
                </td>
              </tr>

              <!-- ICCV 2021-->
              <tr>
                <td><img class="proj_thumb" src="Paper/ICCV21.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Variational Feature Disentangling for Fine-Grained Few-Shot Classification </p>
                  <p class="pub_author">Jingyi Xu, <b>Hieu Le</b>, Mingzhen Huang, ShahRukh Athar, Dimitris Samaras.<br>
                    IEEE International Conference on Computer Vision </i> (<b>ICCV</b>), 2021.<br>
                    [<a href="https://github.com/cvlab-stonybrook/vfd-iccv21" target="_blank">PyTorch Code</a>]
                  </p>
                </td>
              </tr>

              <tr>
                <td><img class="proj_thumb" src="Paper/oneplos.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Aerial-trained deep learning networks for surveying cetaceans from satellite
                    imagery </p>
                  <p class="pub_author"> Alex Borowicz, <b>Hieu Le</b>, Grant Humphries, Georg Nehls, Caroline Hoschle,
                    Vladislav Kosarev, Heather J. Lynch <br>
                    Plos One </i>, 2019.<br>
                    [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0212532">Paper </a>]
                  </p>
                </td>
              </tr>


              <!-- ECCV 2020-->
              <tr>
                <td><img class="proj_thumb" src="Paper/ECCV20.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">From Shadow Segmentation to Shadow Removal </p>
                  <p class="pub_author"> <b>Hieu Le</b>, Dimitris Samaras.<br>
                    European Conference on Computer Vision </i> (<b>ECCV</b>), 2020.<br>
                    [<a href="https://arxiv.org/abs/2008.00267"> Paper </a> <a
                      href="https://www3.cs.stonybrook.edu/~cvl/projects/FSS2SR/index.html">Project Page</a> ]
                  </p>
                </td>
              </tr>

              <!-- ICCV 2019-->
              <tr>
                <td><img class="proj_thumb" src="Paper/ICCV19.png" alt="" height="120px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Shadow Removal via Shadow Image Decomposition </p>
                  <p class="pub_author"> <b>Hieu Le</b>, Dimitris Samaras.<br>
                    International Conference on Computer Vision </i> (<b>ICCV</b>), 2019.<br>
                    [<a href="https://arxiv.org/abs/1908.08628">Paper</a> <a
                      href="https://www3.cs.stonybrook.edu/~cvl/projects/SID/index.html">Project Page</a> ]
                  </p>
                </td>
              </tr>


              <!-- CVPRW 2019-->
              <tr>
                <td><img class="proj_thumb" src="Paper/CVPR2019.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Weakly Labeling the Antarctic: The Penguin Colony Case </p>
                  <p class="pub_author"> <b>Hieu Le</b>, Bento Gonçalves, Dimitris Samaras, Heather Lynch.<br>
                    IEEE Conference on Computer Vision and Pattern Recognition - Workshop </i> (<b>CVPRW</b>), 2019.<br>
                    [<a href="https://arxiv.org/abs/1905.03313">Paper</a> ]
                  </p>
                </td>
              </tr>

              <!-- ECCV 2018-->
              <tr>
                <td><img class="proj_thumb" src="Paper/cvpr18.png" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">A+D Net: Training a Shadow Detector with Adversarial Shadow Attenuation. </p>
                  <p class="pub_author"> <b>Hieu Le</b>, Tomas F. Yago Vicente, Vu Nguyen, Minh Hoai, Dimitris Samaras.
                    <br>
                    European Conference on Computer Vision </i> (<b>ECCV</b>), 2018.<br>
                    [<a href="https://arxiv.org/abs/1712.01361">Paper</a>]
                  </p>
                </td>
              </tr>


              <!-- ECCV 2018 counting-->
              <tr>
                <td><img class="proj_thumb" src="Paper/sm_crowdcounting.jpg" alt="" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Iterative Crowd Counting. </p>
                  <p class="pub_author"> Viresh Ranjan, <b>Hieu Le</b>, and Minh Hoai. <br>
                    European Conference on Computer Vision </i> (<b>ECCV</b>), 2018.<br>
                    [<a href="https://arxiv.org/abs/1807.09959">Paper</a>]
                  </p>
                </td>
              </tr>


              <!-- ICCV 2017 W-->
              <tr>
                <td><img class="proj_thumb" src="Paper/ObjectDiscovery.jpeg" alt="" height="120px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Co-localization with Category-Consistent CNN Features and Geodesic Distance
                    Propagation </p>
                  <p class="pub_author"> <b>Hieu Le</b>, Chen-Ping Yu, Gregory Zelinsky, Dimitris Samaras.<br>
                    International Conference on Computer Vision - Workshop </i> (<b>ICCVW</b>), 2017.<br>
                    [<a href="https://arxiv.org/abs/1612.03236">Paper</a>]
                  </p>
                </td>
              </tr>


              <!-- ACCV 2016 -->
              <tr>
                <td><img class="proj_thumb" src="Paper/preview.jpg" alt="" height="120px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Geodesic Distance Histogram Feature for Video Segmentation </p>
                  <p class="pub_author"> <b>Hieu Le</b>, Vu Nguyen, Chen-Ping Yu, Dimitris Samaras<br>
                    Asian Conference on Computer Vision </i> (<b>ACCV</b>), 2016.<br>
                    <a href="https://link.springer.com/chapter/10.1007/978-3-319-54181-5_18">[link]</a> <a
                      href="Project/ACCV16/geodesic-distance-histogram.pdf">[Paper]</a> <a
                      href="Project/ACCV16/ACCV_poster.pdf">[Poster]</a>
                  </p>
                </td>
              </tr>

              <!-- ICCV 2015 -->
              <tr>
                <td><img class="proj_thumb" src="Paper/PGP.png" alt="" height="120px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Efficient video segmentation using parametric graph partitioning </p>
                  <p class="pub_author"> Chen-Ping Yu, <b>Hieu Le</b>, Gregory Zelinsky, Dimitris Samaras<br>
                    International Conference on Computer Vision </i> (<b>ICCV</b>), 2015.<br>
                    [<a
                      href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yu_Efficient_Video_Segmentation_ICCV_2015_paper.pdf">
                      paper]
                  </p>
                </td>
              </tr>

            </table>
        </div>

        <div id="preprints" class="tab-content" style="margin-left: 20px;">
          <table>
            <table class="imgtable">
              <tr>
                <td><img  class="proj_thumb" src="Paper/talkinghead25.png" alt="Paper Thumbnail" height="100px" /></td>
                <td>
                  <p class="pub_title">Learning Frame-Wise Emotion Intensity for Audio-Driven Talking-Head Generation
                  </p>
                  <p>
                    <a href="https://jingyixu.net/">Jingyi Xu</a>, <b>Hieu Le</b>, Zhixin Shu, Yang Wang, Yi-Hsuan Tsai,
                    Dimitris Samaras, 2025 <br>
                    <a href="https://arxiv.org/pdf/2409.19501" target="_blank">[Preprint]</a>
                    <button class="nice-button" onclick="toggleDescription('desc-talkinghead')"
                      style="margin-left: 10px;">TL;DR</button>
                  </p>
                  <p id="desc-talkinghead" style="display: none; font-style: italic; color: #22710e;">
                    We introduce a novel method for generating talking-head videos driven by audio input, incorporating
                    frame-wise emotion intensity to enhance realism and expressiveness in visual output. The intensity
                    is
                    pseudo-labeled and we introduce a latent space that facilitates generating videos with varied
                    frame-wise intensities.
                  </p>
                </td>
              </tr>
              <!--1 -->
              <tr>
                <td><img  class="proj_thumb" src="Paper/iGres25.png" alt="Paper Thumbnail" height="100px" /></td>
                <td>
                  <p class="pub_title">Instance-Aware Generalized Referring Expression Segmentation</p>
                  <p>
                    <a href="https://eronguyen.me">E-Ro Nguyen</a>, <b>Hieu Le</b>, Dimitris Samaras, Michael Ryoo, 2025
                    <br>
                    <a href="https://arxiv.org/pdf/2411.15087" target="_blank">[Preprint]</a><a
                      href="https://eronguyen.me/InstAlign/"> [Project Page]</a>
                    <button class="nice-button" onclick="toggleDescription('desc-iGres25')"
                      style="margin-left: 10px;">TL;DR</button>
                  </p>
                  <p id="desc-iGres25" style="display: none; font-style: italic; color: #22710e;">
                    We introduces a novel approach to referring expression segmentation with instance-awareness,
                    automatically finding and linking object instances in the image with the textual entities describing
                    them.
                  </p>
                </td>
              </tr>

              <!--  -->
              <tr>
                <td>
                  <img  class="proj_thumb" src="Paper/ibtoken25.png" alt="Paper Thumbnail" height="100px" />
                </td>
                <td>
                  <p class="pub_title">Importance-based Token Merging for Diffusion Models</p>
                  <p>
                    <a href="https://hao-yu-wu.github.io/">Haoyu Wu</a>, Jingyi Xu, <b>Hieu Le</b>, Dimitris Samaras,
                    2025<br>
                    <a href="https://www.arxiv.org/abs/2411.16720" target="_blank">[Preprint]</a> <a
                      href="https://hao-yu-wu.github.io/token_merging/">[Project Page]</a>
                    <button class="nice-button" onclick="toggleDescription('tldr-importance-token')">TL;DR</button>
                  </p>
                  <p id="tldr-importance-token" style="display: none; font-style: italic; color: green;">
                    TL;DR: A novel token-merging method speeds up diffusion models by preserving important tokens,
                    enhancing sample quality with minimal computational cost.
                  </p>
                </td>
              </tr>


              <!-- -->
              <tr>
                <td><img class="proj_thumb" src="Paper/2025/3dcounting.png" alt="Paper Thumbnail" height="100px" />&nbsp;</td>
                <td>
                  <p class="pub_title">Counting Stacked Objects from Multi-View Images</p>
                  <p>
                    Corentin Dumery, Noa Etté, Jingyi Xu, Aoxiang Fan, Ren Li, <b>Hieu Le</b>, Pascal Fua, 2025 <br>
                    <a href="https://arxiv.org/pdf/2411.19149" target="_blank">[Preprint]</a> 
                    <button class="nice-button" onclick="toggleDescription('tldr-3dcounting')">TL;DR</button>
                  </p>
                  <p id="tldr-3dcounting" style="display: none; font-style: italic; color: green;">
                    TL;DR: We do 3D volume reconstruction from multi-view images to count hidden objects
                  </p>
                </td>
              </tr>
              <!--  -->

            <!--  -->
            <tr>
              <td>
                  <img class="proj_thumb" src="Paper/2025/medtet_4d_reconstruction.gif" alt="Paper Thumbnail" height="100px" />&nbsp;
              </td>
              <td>
                  <p class="pub_title">MedTet: An Online Motion Model for 4D Heart Reconstruction</p>
                  <p>
                      <a href="https://scalsol.github.io/">Yihong Chen</a>, Jiancheng Yang, Deniz Sayin Mercadier, <b>Hieu Le</b>, Pascal Fua, 2025<br>
                      <a href="https://arxiv.org/pdf/2412.02589" target="_blank">[Preprint]</a> <a href="https://github.com/Scalso/MedTet" target="_blank">[Code]</a> <a href="https://scalsol.github.io/medtet/">[Project Page]</a>
                      <button class="nice-button" onclick="toggleDescription('tldr-medtet-4d')">TL;DR</button>
                  </p>
                  <p id="tldr-medtet-4d" style="display: none; font-style: italic; color: green;">
                      TL;DR: This paper introduces a versatile framework for reconstructing 3D cardiac motion from limited real-time data, such as 2D slices or even 1D signals. Using a deformable tetrahedral grid, the method ensures anatomically consistent reconstructions suitable for intraoperative scenarios.
                  </p>
              </td>
          </tr>
          
            

            </table>
        </div>

        <script>
          function toggleDescription(id) {
            const desc = document.getElementById(id);
            if (desc.style.display === "none") {
              desc.style.display = "block"; // Show the description
            } else {
              desc.style.display = "none"; // Hide the description
            }
          }
        </script>


        <script>
          function openTab(evt, tabName) {
            // Hide all tab contents
            var tabContents = document.getElementsByClassName('tab-content');
            for (var i = 0; i < tabContents.length; i++) {
              tabContents[i].classList.remove('active');
            }

            // Remove "active" class from all tab links
            var tabLinks = document.getElementsByClassName('tablinks');
            for (var i = 0; i < tabLinks.length; i++) {
              tabLinks[i].classList.remove('active');
            }

            // Show the current tab and add "active" to the clicked button
            document.getElementById(tabName).classList.add('active');
            evt.currentTarget.classList.add('active');
          }
        </script>

        <table>

          <!-- Services -->
          <a id="services" class="anchor"></a>
          <h2>Services</h2>

          <p>Journal Reviewer: </p>
          <font size="2">
            <ul>
              <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
              <li>International Journal of Computer Vision (IJCV)</li>
              <li>IEEE Transactions on Image Processing (TIP)</li>
              <li>Computer Vision and Image Understanding (CVIU)</li>
              <li>Journal of Photogrammetry and Remote Sensing (ISPRS)</li>
            </ul>
          </font>


          <p>Conference Reviewer: </p>
          <font size="2">
            <ul>
              <li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
              <li>International Conference on Computer Vision (ICCV)</li>
              <li>European Conference on Computer Vision (ECCV)</li>
              <li>AAAI Conference on Artificial Intelligence (AAAI)</li>
              <li>Asian Conference on Computer Vision (ACCV)</li>
              <li>The International Conference on Learning Representations (ICLR)</li>
              <li>International Conference on Machine Learning (ICML)</li>
              <li>Conference on Neural Information Processing Systems
                (NeurISP)</li>
            </ul>
          </font>

          <!-- awards -->
          <a id="awards" class="achor"></a>
          <h2>Awards</h2>
          <font size="2">
            <ul>
              <li>DAAD Postdoc-NeT-AI 2024 </li>
              <li>Outstanding Reviewer - ECCV 2020, CVPR 2021, ACCV 2022</li>
              <li>Vietnam Education Foundation Fellowship - 2014</li>
              <li>Microsoft AI for Earth Travel Grant , 2019</li>
              <li>Vietnam NAFOTES Sponsorship, 2014</li>
              <li>Silver Medal - Vietnam National Olympiad in Informatics</li>
            </ul>
          </font>


          <div id="footer">
            <div id="footer-text">
              <!--
  All Rights Reserved. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
  -->

            </div>
          </div>
          <!-- Start of StatCounter Code for Default Guide -->
          <a href="https://clustrmaps.com/site/1adbx" title="Visit tracker"><img
              src="//www.clustrmaps.com/map_v2.png?d=cp3sdODWAOp83IWwG9oWWhU6O7-7BY0tlIrNTy-vheI&cl=ffffff" /></a>
          <script type="text/javascript">
            var sc_project = 11304989;
            var sc_invisible = 0;
            var sc_security = "dada4aa6";
            var scJsHost = (("https:" == document.location.protocol) ?
              "https://secure." : "http://www.");
            document.write("<sc" + "ript type='text/javascript' src='" +
              scJsHost +
              "statcounter.com/counter/counter.js'></" + "script>");
          </script>
          <noscript>
            <div class="statcounter"><a title="free web stats" href="http://statcounter.com/" target="_blank"><img
                  class="statcounter" src="//c.statcounter.com/11304989/0/dada4aa6/0/" alt="free
  web stats"></a></div>
          </noscript>
          <!-- End of StatCounter Code for Default Guide -->


      </body>

</html>
